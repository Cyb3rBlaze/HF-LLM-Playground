{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016188859939575195,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6146439d680a4a28bea3bac49ce9cb0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RWModel(\n",
       "  (word_embeddings): Embedding(65024, 4544)\n",
       "  (h): ModuleList(\n",
       "    (0-31): 32 x DecoderLayer(\n",
       "      (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): Attention(\n",
       "        (maybe_rotary): RotaryEmbedding()\n",
       "        (query_key_value): Linear(in_features=4544, out_features=4672, bias=False)\n",
       "        (dense): Linear(in_features=4544, out_features=4544, bias=False)\n",
       "        (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (dense_h_to_4h): Linear(in_features=4544, out_features=18176, bias=False)\n",
       "        (act): GELU(approximate='none')\n",
       "        (dense_4h_to_h): Linear(in_features=18176, out_features=4544, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "\n",
    "        self.word_embeddings = model.transformer.word_embeddings\n",
    "\n",
    "        tranformer_blocks = model.transformer.h\n",
    "\n",
    "        used_blocks = []\n",
    "\n",
    "        for _, block in enumerate(tranformer_blocks):\n",
    "            used_blocks += [block]\n",
    "            if _ > 20:\n",
    "                break\n",
    "\n",
    "        self.used_blocks = torch.nn.ModuleList(used_blocks)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.word_embeddings(x)\n",
    "\n",
    "        for block in self.used_blocks:\n",
    "            if type(output) is tuple:\n",
    "                output = block(output[0], alibi=None, attention_mask=torch.ones((1, len(x))))\n",
    "            else:\n",
    "                output = block(output, alibi=None, attention_mask=torch.ones((1, len(x))))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureExtractor(\n",
       "  (word_embeddings): Embedding(65024, 4544)\n",
       "  (used_blocks): ModuleList(\n",
       "    (0-21): 22 x DecoderLayer(\n",
       "      (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): Attention(\n",
       "        (maybe_rotary): RotaryEmbedding()\n",
       "        (query_key_value): Linear(in_features=4544, out_features=4672, bias=False)\n",
       "        (dense): Linear(in_features=4544, out_features=4544, bias=False)\n",
       "        (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (dense_h_to_4h): Linear(in_features=4544, out_features=18176, bias=False)\n",
       "        (act): GELU(approximate='none')\n",
       "        (dense_4h_to_h): Linear(in_features=18176, out_features=4544, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor()\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 4544])\n",
      "torch.Size([1, 4544])\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(\"Hello! How are you doing today?\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "output = feature_extractor(encoding[\"input_ids\"])[0]\n",
    "print(output.shape)\n",
    "output = torch.mean(output, dim=1)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:39<00:00,  3.12s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! How are you doing today?\\nI'm doing well, thank you. I'm just getting ready to head out for a walk. Do you have any plans for the day?\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer(\"Hello! How are you doing today?\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "max_length = 40\n",
    "start_input_size = len(encoding[\"input_ids\"][0])\n",
    "kwargs = {'max_length': 40, 'do_sample': True, 'top_k': 10, 'num_return_sequences': 1, 'eos_token_id': 11}\n",
    "\n",
    "# greedy search not sampling\n",
    "for i in tqdm(range(max_length-start_input_size)):\n",
    "    inputs = model.prepare_inputs_for_generation(**encoding, **kwargs)\n",
    "    output = model(**inputs, return_dict=True, output_attentions=model.generation_config.output_attentions, output_hidden_states=model.generation_config.output_hidden_states)\n",
    "    next_token_idx = torch.argmax(output[\"logits\"][:, -1, :], -1)\n",
    "    encoding[\"input_ids\"] = torch.hstack((encoding[\"input_ids\"], next_token_idx.view(1, -1)))\n",
    "    encoding[\"token_type_ids\"] = torch.hstack((encoding[\"token_type_ids\"], torch.zeros((1, 1))))\n",
    "    encoding[\"attention_mask\"] = torch.hstack((encoding[\"attention_mask\"], torch.ones((1, 1))))\n",
    "\n",
    "token_ids = encoding[\"input_ids\"][0]\n",
    "tokenizer.decode(list(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RWForCausalLM(\n",
      "  (transformer): None\n",
      "  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)\n",
      ")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/vm/falcon-test/test.ipynb Cell 9\u001b[0m in \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/falcon-test/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39mtransformer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/falcon-test/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(model)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/falcon-test/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mmodules:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ubuntu/vm/falcon-test/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(module)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not iterable"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(\"Hello! How are you doing today?\", truncation=True, return_tensors=\"pt\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falcon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
